#!/usr/bin/env python3
"""
HTML to Markdown Scraper - Versi√≥n Profesional Mejorada
======================================================

Extractor profesional de contenido HTML a Markdown usando Playwright.

Mejoras implementadas:
- Manejo robusto de errores con logging profesional
- Configuraci√≥n flexible desde archivos JSON
- Optimizaci√≥n de recursos (reutilizaci√≥n de navegador)
- Validaciones de entrada completas
- Procesamiento paralelo opcional
- M√©tricas y estad√≠sticas detalladas
- Interfaz de l√≠nea de comandos avanzada
- Nombres de archivo inteligentes

Autor: Versi√≥n mejorada del c√≥digo original
Fecha: 2025
"""

import asyncio
import json
import logging
import os
import time
import argparse
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Dict, Tuple
from dataclasses import dataclass, asdict

from playwright.async_api import async_playwright, Browser, BrowserContext
from markdownify import markdownify as md


@dataclass
class EstadisticasProcesamiento:
    """Clase para almacenar estad√≠sticas del procesamiento."""
    inicio: float = 0
    fin: float = 0
    archivos_procesados: int = 0
    archivos_fallidos: int = 0
    total_palabras: int = 0
    total_caracteres: int = 0
    urls_procesadas: List[str] = None
    urls_fallidas: List[str] = None
    
    def __post_init__(self):
        if self.urls_procesadas is None:
            self.urls_procesadas = []
        if self.urls_fallidas is None:
            self.urls_fallidas = []
    
    @property
    def duracion(self) -> float:
        """Calcula la duraci√≥n total del procesamiento."""
        return self.fin - self.inicio if self.fin > self.inicio else 0
    
    @property
    def total_archivos(self) -> int:
        """Retorna el total de archivos procesados."""
        return self.archivos_procesados + self.archivos_fallidos
    
    @property
    def tasa_exito(self) -> float:
        """Calcula la tasa de √©xito en porcentaje."""
        if self.total_archivos == 0:
            return 0
        return (self.archivos_procesados / self.total_archivos) * 100
    
    def imprimir_resumen(self):
        """Imprime un resumen detallado de las estad√≠sticas."""
        print("\n" + "="*60)
        print("üìä ESTAD√çSTICAS FINALES DEL PROCESAMIENTO")
        print("="*60)
        print(f"üìÑ Total de archivos procesados: {self.total_archivos}")
        print(f"‚úÖ Exitosos: {self.archivos_procesados}")
        print(f"‚ùå Fallidos: {self.archivos_fallidos}")
        print(f"üìà Tasa de √©xito: {self.tasa_exito:.1f}%")
        print(f"üìù Total de palabras extra√≠das: {self.total_palabras:,}")
        print(f"üìè Total de caracteres: {self.total_caracteres:,}")
        print(f"‚è±Ô∏è Tiempo total: {self.duracion:.2f} segundos")
        
        if self.archivos_procesados > 0:
            tiempo_promedio = self.duracion / self.archivos_procesados
            palabras_promedio = self.total_palabras // self.archivos_procesados
            print(f"‚ö° Tiempo promedio por archivo: {tiempo_promedio:.2f}s")
            print(f"üìä Palabras promedio por archivo: {palabras_promedio:,}")
        
        if self.urls_fallidas:
            print(f"\n‚ùå URLs que fallaron:")
            for url in self.urls_fallidas:
                print(f"   ‚Ä¢ {url}")
        
        print("="*60)


class HTMLToMarkdownScraper:
    """
    Extractor profesional de contenido HTML a Markdown usando Playwright.
    
    Esta clase maneja la extracci√≥n de contenido de archivos HTML locales o URLs remotas,
    convierte el contenido a Markdown y guarda los archivos con configuraci√≥n flexible.
    """
    
    def __init__(self, config_path: str = "config.json"):
        """
        Inicializa el scraper con configuraci√≥n desde archivo.
        
        Args:
            config_path: Ruta al archivo de configuraci√≥n JSON
        """
        self.config = self._load_config(config_path)
        self.logger = self._setup_logging()
        self.stats = EstadisticasProcesamiento()
        self.browser: Optional[Browser] = None
        
        self.logger.info("üöÄ HTML to Markdown Scraper inicializado")
        self.logger.info(f"üìÅ Configuraci√≥n cargada desde: {config_path}")
    
    def _load_config(self, config_path: str) -> Dict:
        """
        Carga la configuraci√≥n desde archivo JSON o usa valores por defecto.
        
        Args:
            config_path: Ruta al archivo de configuraci√≥n
            
        Returns:
            Diccionario con la configuraci√≥n cargada
        """
        default_config = {
            "urls": [
                "file:///C:/Users/frlpi/OneDrive/Documentos/ADSO/Fase%201%20-%20Analisis/Actividad%20de%20proyecto%201/1.%20Caracterizaci%C3%B3n%20de%20procesos/index.html#/introduccion",
                "file:///C:/Users/frlpi/OneDrive/Documentos/ADSO/Fase%201%20-%20Analisis/Actividad%20de%20proyecto%201/1.%20Caracterizaci%C3%B3n%20de%20procesos/index.html#/curso/tema1",
                "file:///C:/Users/frlpi/OneDrive/Documentos/ADSO/Fase%201%20-%20Analisis/Actividad%20de%20proyecto%201/1.%20Caracterizaci%C3%B3n%20de%20procesos/index.html#/curso/tema2",
                "file:///C:/Users/frlpi/OneDrive/Documentos/ADSO/Fase%201%20-%20Analisis/Actividad%20de%20proyecto%201/1.%20Caracterizaci%C3%B3n%20de%20procesos/index.html#/curso/tema3",
                "file:///C:/Users/frlpi/OneDrive/Documentos/ADSO/Fase%201%20-%20Analisis/Actividad%20de%20proyecto%201/1.%20Caracterizaci%C3%B3n%20de%20procesos/index.html#/curso/tema4"
            ],
            "output_dir": "C:/Users/frlpi/OneDrive/Escritorio/scraper",
            "options": {
                "headless": True,
                "wait_until": "networkidle",
                "timeout": 30000,
                "parallel": False,
                "max_concurrent": 3,
                "delay_between_requests": 1000,
                "retry_attempts": 2
            },
            "markdown": {
                "strip_elements": ["script", "style", "nav", "footer", "aside", "header"],
                "file_extension": ".mdx",
                "naming_pattern": "contenido_{index}",
                "smart_naming": True,
                "clean_excessive_whitespace": True
            },
            "logging": {
                "level": "INFO",
                "console": True,
                "file": True,
                "log_dir": "logs"
            }
        }
        
        config_file = Path(config_path)
        if config_file.exists():
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                    # Fusi√≥n profunda de configuraciones
                    self._deep_merge_config(default_config, user_config)
                    print(f"‚úÖ Configuraci√≥n cargada desde: {config_path}")
            except json.JSONDecodeError as e:
                print(f"‚ö†Ô∏è Error en formato JSON de {config_path}: {e}")
                print("Usando configuraci√≥n por defecto.")
            except Exception as e:
                print(f"‚ö†Ô∏è Error cargando configuraci√≥n: {e}")
                print("Usando configuraci√≥n por defecto.")
        else:
            print(f"üìÑ Archivo de configuraci√≥n no encontrado: {config_path}")
            print("Usando configuraci√≥n por defecto.")
        
        return default_config
    
    def _deep_merge_config(self, default: Dict, user: Dict) -> None:
        """
        Fusiona recursivamente la configuraci√≥n del usuario con la por defecto.
        
        Args:
            default: Configuraci√≥n por defecto (se modifica in-place)
            user: Configuraci√≥n del usuario
        """
        for key, value in user.items():
            if key in default and isinstance(default[key], dict) and isinstance(value, dict):
                self._deep_merge_config(default[key], value)
            else:
                default[key] = value
    
    def _setup_logging(self) -> logging.Logger:
        """
        Configura el sistema de logging profesional.
        
        Returns:
            Logger configurado para el scraper
        """
        log_config = self.config.get('logging', {})
        
        # Crear directorio de logs
        log_dir = Path(log_config.get('log_dir', 'logs'))
        log_dir.mkdir(exist_ok=True)
        
        # Configurar nivel de logging
        log_level = getattr(logging, log_config.get('level', 'INFO').upper())
        
        # Configurar handlers
        handlers = []
        
        if log_config.get('console', True):
            console_handler = logging.StreamHandler()
            console_handler.setFormatter(
                logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            )
            handlers.append(console_handler)
        
        if log_config.get('file', True):
            log_filename = f"scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
            log_path = log_dir / log_filename
            file_handler = logging.FileHandler(log_path, encoding='utf-8')
            file_handler.setFormatter(
                logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s')
            )
            handlers.append(file_handler)
        
        # Configurar logger
        logger = logging.getLogger('HTMLScraper')
        logger.setLevel(log_level)
        
        # Remover handlers existentes para evitar duplicados
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)
        
        for handler in handlers:
            logger.addHandler(handler)
        
        return logger
    
    def _validate_config(self) -> bool:
        """
        Valida la configuraci√≥n antes de procesar.
        
        Returns:
            True si la configuraci√≥n es v√°lida, False en caso contrario
        """
        errores = []
        
        # Validar URLs
        if not self.config.get('urls'):
            errores.append("No se han especificado URLs para procesar")
        elif not isinstance(self.config['urls'], list):
            errores.append("Las URLs deben estar en formato de lista")
        
        # Validar directorio de salida
        if not self.config.get('output_dir'):
            errores.append("No se ha especificado directorio de salida")
        
        # Validar URLs de archivos locales
        urls_invalidas = []
        for url in self.config.get('urls', []):
            if url.startswith('file://'):
                # Limpiar la URL para obtener la ruta del archivo
                file_path = url.replace('file:///', '').split('#')[0]
                # Decodificar caracteres especiales en la URL
                file_path = file_path.replace('%20', ' ').replace('%C3%B3', '√≥')
                
                if not Path(file_path).exists():
                    urls_invalidas.append(f"Archivo no encontrado: {file_path}")
        
        if urls_invalidas:
            errores.extend(urls_invalidas)
        
        # Validar permisos de escritura
        try:
            output_path = Path(self.config['output_dir'])
            output_path.mkdir(parents=True, exist_ok=True)
            test_file = output_path / ".test_write_permission"
            test_file.write_text("test")
            test_file.unlink()
        except Exception as e:
            errores.append(f"Sin permisos de escritura en: {self.config['output_dir']} ({e})")
        
        # Mostrar errores si los hay
        if errores:
            self.logger.error("‚ùå Errores de configuraci√≥n encontrados:")
            for error in errores:
                self.logger.error(f"   ‚Ä¢ {error}")
            return False
        
        self.logger.info("‚úÖ Configuraci√≥n validada correctamente")
        return True
    
    def _validate_urls(self) -> List[str]:
        """
        Valida y filtra las URLs que sean accesibles.
        
        Returns:
            Lista de URLs v√°lidas
        """
        valid_urls = []
        
        for url in self.config['urls']:
            try:
                if url.startswith('file://'):
                    # Para archivos locales, verificar que existan
                    file_path = url.replace('file:///', '').split('#')[0]
                    file_path = file_path.replace('%20', ' ').replace('%C3%B3', '√≥')
                    
                    if Path(file_path).exists():
                        valid_urls.append(url)
                        self.logger.debug(f"‚úÖ URL v√°lida: {url}")
                    else:
                        self.logger.warning(f"‚ö†Ô∏è Archivo no encontrado: {file_path}")
                else:
                    # Para URLs remotas, asumir v√°lidas (se verificar√°n al acceder)
                    valid_urls.append(url)
                    self.logger.debug(f"‚úÖ URL remota: {url}")
            except Exception as e:
                self.logger.error(f"‚ùå Error validando URL {url}: {e}")
        
        self.logger.info(f"üìä URLs v√°lidas encontradas: {len(valid_urls)}/{len(self.config['urls'])}")
        return valid_urls
    
    async def _init_browser(self) -> None:
        """Inicializa el navegador una sola vez para reutilizaci√≥n."""
        if self.browser is None:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(
                headless=self.config['options']['headless']
            )
            self.logger.info("üåê Navegador inicializado")
    
    async def _close_browser(self) -> None:
        """Cierra el navegador y limpia recursos."""
        if self.browser:
            await self.browser.close()
            await self.playwright.stop()
            self.browser = None
            self.logger.info("üîí Navegador cerrado")
    
    async def _extract_content_safe(self, url: str, retry_count: int = 0) -> Optional[str]:
        """
        Extrae contenido HTML de forma segura con manejo de errores y reintentos.
        
        Args:
            url: URL a procesar
            retry_count: N√∫mero de intento actual
            
        Returns:
            Contenido HTML extra√≠do o None si falla
        """
        context = None
        page = None
        max_retries = self.config['options'].get('retry_attempts', 2)
        
        try:
            await self._init_browser()
            
            context = await self.browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            )
            page = await context.new_page()
            
            self.logger.info(f"üåê Accediendo a: {url}")
            
            # Configurar timeouts
            timeout = self.config['options'].get('timeout', 30000)
            wait_until = self.config['options'].get('wait_until', 'networkidle')
            
            await page.goto(url, wait_until=wait_until, timeout=timeout)
            
            # Esperar un poco m√°s para contenido din√°mico
            await page.wait_for_timeout(2000)
            
            content = await page.content()
            
            if content and len(content) > 100:  # Verificar que el contenido no est√© vac√≠o
                self.logger.info(f"‚úÖ Contenido extra√≠do: {len(content):,} caracteres")
                return content
            else:
                self.logger.warning(f"‚ö†Ô∏è Contenido sospechosamente corto: {len(content) if content else 0} caracteres")
                return None
                
        except Exception as e:
            self.logger.error(f"‚ùå Error extrayendo contenido de {url}: {type(e).__name__}: {e}")
            
            # Reintentar si no se ha alcanzado el m√°ximo
            if retry_count < max_retries:
                self.logger.info(f"üîÑ Reintentando ({retry_count + 1}/{max_retries})...")
                await asyncio.sleep(2)  # Esperar antes de reintentar
                return await self._extract_content_safe(url, retry_count + 1)
            
            return None
            
        finally:
            if page:
                await page.close()
            if context:
                await context.close()
    
    def _generate_smart_filename(self, url: str, index: int) -> str:
        """
        Genera nombres de archivo inteligentes basados en el contenido de la URL.
        
        Args:
            url: URL a procesar
            index: √çndice secuencial
            
        Returns:
            Nombre de archivo generado
        """
        pattern = self.config['markdown']['naming_pattern']
        extension = self.config['markdown']['file_extension']
        smart_naming = self.config['markdown'].get('smart_naming', True)
        
        if smart_naming and '#' in url:
            # Extraer el fragmento de la URL
            fragment = url.split('#')[-1]
            # Limpiar caracteres especiales
            fragment = fragment.replace('/', '_').replace('\\', '_')
            fragment = ''.join(c for c in fragment if c.isalnum() or c in '_-')
            filename = f"{fragment}_{index:02d}"
        else:
            filename = pattern.format(index=index)
        
        return f"{filename}{extension}"
    
    def _convert_to_markdown(self, html_content: str) -> str:
        """
        Convierte HTML a Markdown con opciones configurables.
        
        Args:
            html_content: Contenido HTML a convertir
            
        Returns:
            Contenido convertido a Markdown
        """
        if not html_content:
            return ""
        
        try:
            markdown_config = self.config['markdown']
            
            markdown_content = md(
                html_content,
                strip=markdown_config.get('strip_elements', ['script', 'style']),
                convert=['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'ul', 'ol', 'li', 
                        'a', 'strong', 'em', 'code', 'pre', 'blockquote', 
                        'table', 'tr', 'td', 'th', 'img'],
                heading_style='ATX'
            )
            
            # Limpiar contenido si est√° habilitado
            if markdown_config.get('clean_excessive_whitespace', True):
                markdown_content = self._clean_markdown(markdown_content)
            
            return markdown_content
            
        except Exception as e:
            self.logger.error(f"‚ùå Error convirtiendo a Markdown: {e}")
            return ""
    
    def _clean_markdown(self, markdown: str) -> str:
        """
        Limpia y optimiza el contenido Markdown.
        
        Args:
            markdown: Contenido Markdown a limpiar
            
        Returns:
            Contenido Markdown limpio
        """
        if not markdown:
            return ""
        
        # Separar en l√≠neas
        lines = markdown.split('\n')
        cleaned_lines = []
        empty_count = 0
        
        for line in lines:
            line = line.rstrip()  # Quitar espacios al final
            
            if line.strip():
                cleaned_lines.append(line)
                empty_count = 0
            else:
                empty_count += 1
                # M√°ximo 2 l√≠neas vac√≠as consecutivas
                if empty_count <= 2:
                    cleaned_lines.append('')
        
        # Unir l√≠neas y limpiar el inicio y final
        result = '\n'.join(cleaned_lines).strip()
        
        # Reemplazar m√∫ltiples espacios por uno solo
        import re
        result = re.sub(r' +', ' ', result)
        
        return result
    
    def _save_markdown_file(self, content: str, filename: str) -> bool:
        """
        Guarda contenido Markdown en archivo.
        
        Args:
            content: Contenido a guardar
            filename: Nombre del archivo
            
        Returns:
            True si se guard√≥ exitosamente, False en caso contrario
        """
        if not content or not content.strip():
            self.logger.warning(f"‚ö†Ô∏è Contenido vac√≠o para {filename}")
            return False
        
        try:
            output_path = Path(self.config['output_dir']) / filename
            
            # Verificar si archivo existe
            if output_path.exists():
                self.logger.info(f"üìù Sobrescribiendo archivo existente: {filename}")
            
            # Guardar archivo
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            # Calcular m√©tricas
            word_count = len(content.split())
            char_count = len(content)
            
            # Actualizar estad√≠sticas
            self.stats.total_palabras += word_count
            self.stats.total_caracteres += char_count
            
            self.logger.info(f"‚úÖ Archivo guardado: {filename} ({word_count:,} palabras, {char_count:,} caracteres)")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Error guardando archivo {filename}: {e}")
            return False
    
    async def _process_single_url(self, url: str, index: int) -> bool:
        """
        Procesa una URL individual completamente.
        
        Args:
            url: URL a procesar
            index: √çndice secuencial
            
        Returns:
            True si se proces√≥ exitosamente, False en caso contrario
        """
        try:
            self.logger.info(f"üìÑ Procesando [{index}]: {url}")
            
            # Extraer contenido HTML
            html_content = await self._extract_content_safe(url)
            if not html_content:
                self.stats.archivos_fallidos += 1
                self.stats.urls_fallidas.append(url)
                return False
            
            # Convertir a Markdown
            markdown_content = self._convert_to_markdown(html_content)
            if not markdown_content:
                self.logger.error(f"‚ùå Fallo en conversi√≥n a Markdown para: {url}")
                self.stats.archivos_fallidos += 1
                self.stats.urls_fallidas.append(url)
                return False
            
            # Generar nombre de archivo y guardar
            filename = self._generate_smart_filename(url, index)
            success = self._save_markdown_file(markdown_content, filename)
            
            if success:
                self.stats.archivos_procesados += 1
                self.stats.urls_procesadas.append(url)
                return True
            else:
                self.stats.archivos_fallidos += 1
                self.stats.urls_fallidas.append(url)
                return False
                
        except Exception as e:
            self.logger.error(f"‚ùå Error procesando {url}: {e}")
            self.stats.archivos_fallidos += 1
            self.stats.urls_fallidas.append(url)
            return False
    
    async def run_sequential(self) -> None:
        """Ejecuta procesamiento secuencial (recomendado para archivos locales)."""
        self.logger.info("üöÄ Iniciando procesamiento secuencial")
        
        # Validar configuraci√≥n
        if not self._validate_config():
            self.logger.error("‚ùå Configuraci√≥n inv√°lida. Abortando procesamiento.")
            return
        
        # Obtener URLs v√°lidas
        valid_urls = self._validate_urls()
        if not valid_urls:
            self.logger.error("‚ùå No hay URLs v√°lidas para procesar")
            return
        
        self.logger.info(f"üìä Iniciando procesamiento de {len(valid_urls)} URLs")
        self.stats.inicio = time.time()
        
        try:
            for i, url in enumerate(valid_urls, 1):
                await self._process_single_url(url, i)
                
                # Pausa entre requests si est√° configurada
                delay = self.config['options'].get('delay_between_requests', 1000)
                if delay > 0 and i < len(valid_urls):  # No pausar despu√©s del √∫ltimo
                    await asyncio.sleep(delay / 1000.0)  # Convertir ms a segundos
                    
        except KeyboardInterrupt:
            self.logger.warning("‚èπÔ∏è Procesamiento interrumpido por el usuario")
        except Exception as e:
            self.logger.error(f"‚ùå Error fatal durante procesamiento: {e}")
        finally:
            await self._close_browser()
            self.stats.fin = time.time()
            self._print_final_stats()
    
    async def run_parallel(self, max_concurrent: Optional[int] = None) -> None:
        """
        Ejecuta procesamiento paralelo (recomendado para URLs remotas).
        
        Args:
            max_concurrent: N√∫mero m√°ximo de tareas concurrentes
        """
        self.logger.info("üöÄ Iniciando procesamiento paralelo")
        
        # Validar configuraci√≥n
        if not self._validate_config():
            self.logger.error("‚ùå Configuraci√≥n inv√°lida. Abortando procesamiento.")
            return
        
        # Obtener URLs v√°lidas
        valid_urls = self._validate_urls()
        if not valid_urls:
            self.logger.error("‚ùå No hay URLs v√°lidas para procesar")
            return
        
        # Configurar concurrencia
        if max_concurrent is None:
            max_concurrent = self.config['options'].get('max_concurrent', 3)
        
        self.logger.info(f"üìä Iniciando procesamiento paralelo de {len(valid_urls)} URLs (max {max_concurrent} concurrentes)")
        self.stats.inicio = time.time()
        
        # Sem√°foro para controlar concurrencia
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def process_with_semaphore(url: str, index: int) -> bool:
            async with semaphore:
                return await self._process_single_url(url, index)
        
        try:
            # Crear tareas para todas las URLs
            tasks = [
                process_with_semaphore(url, i) 
                for i, url in enumerate(valid_urls, 1)
            ]
            
            # Ejecutar todas las tareas
            await asyncio.gather(*tasks, return_exceptions=True)
                
        except KeyboardInterrupt:
            self.logger.warning("‚èπÔ∏è Procesamiento interrumpido por el usuario")
        except Exception as e:
            self.logger.error(f"‚ùå Error fatal durante procesamiento paralelo: {e}")
        finally:
            await self._close_browser()
            self.stats.fin = time.time()
            self._print_final_stats()
    
    def _print_final_stats(self) -> None:
        """Imprime estad√≠sticas finales del procesamiento."""
        self.stats.imprimir_resumen()
        self.logger.info("üèÅ Procesamiento completado")
        
        # Guardar estad√≠sticas en archivo JSON
        try:
            stats_file = Path(self.config['output_dir']) / "estadisticas_procesamiento.json"
            with open(stats_file, 'w', encoding='utf-8') as f:
                # Convertir dataclass a dict, excluyendo campos que no son serializables
                stats_dict = asdict(self.stats)
                stats_dict['fecha_procesamiento'] = datetime.now().isoformat()
                stats_dict['configuracion_utilizada'] = self.config
                json.dump(stats_dict, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"üìä Estad√≠sticas guardadas en: {stats_file}")
        except Exception as e:
            self.logger.error(f"‚ùå Error guardando estad√≠sticas: {e}")


def create_sample_config(config_path: str = "config_ejemplo.json") -> None:
    """
    Crea un archivo de configuraci√≥n de ejemplo.
    
    Args:
        config_path: Ruta donde crear el archivo de configuraci√≥n
    """
    sample_config = {
        "urls": [
            "file:///C:/ruta/a/tu/archivo1.html",
            "file:///C:/ruta/a/tu/archivo2.html",
            "https://ejemplo.com/pagina1",
            "https://ejemplo.com/pagina2"
        ],
        "output_dir": "salida_markdown",
        "options": {
            "headless": True,
            "wait_until": "networkidle",
            "timeout": 30000,
            "parallel": False,
            "max_concurrent": 3,
            "delay_between_requests": 1000,
            "retry_attempts": 2
        },
        "markdown": {
            "strip_elements": ["script", "style", "nav", "footer", "aside", "header"],
            "file_extension": ".md",
            "naming_pattern": "contenido_{index}",
            "smart_naming": True,
            "clean_excessive_whitespace": True
        },
        "logging": {
            "level": "INFO",
            "console": True,
            "file": True,
            "log_dir": "logs"
        }
    }
    
    try:
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(sample_config, f, indent=2, ensure_ascii=False)
        
        print(f"üìÑ Archivo de configuraci√≥n de ejemplo creado: {config_path}")
        print("‚úèÔ∏è Edita este archivo para personalizar tu configuraci√≥n.")
        
    except Exception as e:
        print(f"‚ùå Error creando archivo de configuraci√≥n: {e}")


def parse_arguments() -> argparse.Namespace:
    """
    Analiza los argumentos de l√≠nea de comandos.
    
    Returns:
        Argumentos analizados
    """
    parser = argparse.ArgumentParser(
        description='HTML to Markdown Scraper Profesional',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ejemplos de uso:
  %(prog)s --create-config
  %(prog)s --config mi_config.json
  %(prog)s --config config.json --parallel
  %(prog)s --urls "file:///archivo1.html" "file:///archivo2.html" --output "salida"
        """
    )
    
    parser.add_argument(
        '--config', 
        default='config.json',
        help='Archivo de configuraci√≥n JSON (default: config.json)'
    )
    
    parser.add_argument(
        '--create-config', 
        action='store_true',
        help='Crear archivo de configuraci√≥n de ejemplo'
    )
    
    parser.add_argument(
        '--parallel', 
        action='store_true',
        help='Usar procesamiento paralelo'
    )
    
    parser.add_argument(
        '--urls', 
        nargs='+',
        help='Lista de URLs a procesar (sobrescribe configuraci√≥n)'
    )
    
    parser.add_argument(
        '--output', 
        help='Directorio de salida (sobrescribe configuraci√≥n)'
    )
    
    parser.add_argument(
        '--headless', 
        action='store_true',
        help='Ejecutar navegador en modo headless'
    )
    
    parser.add_argument(
        '--verbose', 
        action='store_true',
        help='Activar logging detallado (DEBUG)'
    )
    
    return parser.parse_args()


async def main() -> None:
    """Funci√≥n principal mejorada con manejo completo de argumentos."""
    args = parse_arguments()
    
    # Crear configuraci√≥n de ejemplo si se solicita
    if args.create_config:
        create_sample_config()
        return
    
    try:
        # Inicializar scraper
        scraper = HTMLToMarkdownScraper(args.config)
        
        # Sobrescribir configuraci√≥n con argumentos de l√≠nea de comandos
        if args.urls:
            scraper.config['urls'] = args.urls
            scraper.logger.info(f"üìù URLs sobrescritas desde l√≠nea de comandos: {len(args.urls)} URLs")
        
        if args.output:
            scraper.config['output_dir'] = args.output
            scraper.logger.info(f"üìÅ Directorio de salida sobrescrito: {args.output}")
        
        if args.headless:
            scraper.config['options']['headless'] = True
        
        if args.verbose:
            scraper.config['logging']['level'] = 'DEBUG'
            scraper.logger.setLevel(logging.DEBUG)
            scraper.logger.debug("üîç Modo verbose activado")
        
        # Ejecutar procesamiento
        if args.parallel or scraper.config['options'].get('parallel', False):
            await scraper.run_parallel()
        else:
            await scraper.run_sequential()
            
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Procesamiento interrumpido por el usuario")
    except Exception as e:
        print(f"\n‚ùå Error fatal: {e}")
        logging.error(f"Error fatal: {e}", exc_info=True)


if __name__ == "__main__":
    # Verificar dependencias
    try:
        import playwright
        import markdownify
    except ImportError as e:
        print(f"‚ùå Dependencia faltante: {e}")
        print("üì¶ Instala las dependencias con:")
        print("   pip install playwright markdownify")
        print("   playwright install")
        exit(1)
    
    # Ejecutar programa principal
    asyncio.run(main())
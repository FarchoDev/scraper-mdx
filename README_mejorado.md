# ğŸš€ HTML to Markdown Scraper - VersiÃ³n Profesional

Extractor profesional de contenido HTML a Markdown usando Playwright con todas las mejores prÃ¡cticas implementadas.

## âœ¨ CaracterÃ­sticas Principales

### ğŸ”§ **Mejoras Implementadas**
- âœ… **ConfiguraciÃ³n flexible** - Archivos JSON configurables
- âœ… **Manejo robusto de errores** - Try-catch en todas las operaciones
- âœ… **Logging profesional** - Logs detallados en archivo y consola
- âœ… **ValidaciÃ³n completa** - VerificaciÃ³n de archivos y permisos
- âœ… **OptimizaciÃ³n de recursos** - ReutilizaciÃ³n inteligente del navegador
- âœ… **EstadÃ­sticas detalladas** - MÃ©tricas completas de procesamiento
- âœ… **Procesamiento paralelo** - OpciÃ³n para URLs remotas
- âœ… **Nombres inteligentes** - GeneraciÃ³n automÃ¡tica basada en contenido
- âœ… **Limpieza avanzada** - EliminaciÃ³n inteligente de contenido excesivo
- âœ… **CLI avanzada** - Interfaz de lÃ­nea de comandos completa
- âœ… **Reintentos automÃ¡ticos** - Manejo de fallos temporales
- âœ… **ConfiguraciÃ³n por capas** - Sobrescritura flexible de opciones

## ğŸ“¦ InstalaciÃ³n

### Paso 1: Instalar Dependencias
```bash
pip install playwright markdownify
playwright install
```

### Paso 2: Descargar el Script
Guarda el archivo `html_scraper_mejorado.py` en tu directorio de trabajo.

## ğŸš€ Uso RÃ¡pido

### 1. Crear ConfiguraciÃ³n de Ejemplo
```bash
python html_scraper_mejorado.py --create-config
```

### 2. Editar ConfiguraciÃ³n
Edita el archivo `config_ejemplo.json` generado con tus URLs y preferencias:

```json
{
  "urls": [
    "file:///C:/ruta/a/tu/archivo1.html",
    "file:///C:/ruta/a/tu/archivo2.html"
  ],
  "output_dir": "salida_markdown",
  "options": {
    "headless": true,
    "timeout": 30000,
    "parallel": false
  }
}
```

### 3. Ejecutar el Scraper
```bash
# Procesamiento secuencial (recomendado para archivos locales)
python html_scraper_mejorado.py --config config_ejemplo.json

# Procesamiento paralelo (para URLs remotas)
python html_scraper_mejorado.py --config config_ejemplo.json --parallel
```

## ğŸ¯ Ejemplos de Uso

### Uso BÃ¡sico con ConfiguraciÃ³n
```bash
python html_scraper_mejorado.py --config mi_config.json
```

### Sobrescribir URLs desde LÃ­nea de Comandos
```bash
python html_scraper_mejorado.py --urls "file:///archivo1.html" "file:///archivo2.html" --output "mi_salida"
```

### Modo Verbose para Debugging
```bash
python html_scraper_mejorado.py --config config.json --verbose
```

### Procesamiento Paralelo para URLs Remotas
```bash
python html_scraper_mejorado.py --config config.json --parallel
```

## âš™ï¸ ConfiguraciÃ³n Detallada

### Estructura del Archivo de ConfiguraciÃ³n

```json
{
  "urls": [
    "file:///ruta/local/archivo.html",
    "https://sitio-web.com/pagina"
  ],
  "output_dir": "salida_markdown",
  "options": {
    "headless": true,              // Navegador sin ventana
    "wait_until": "networkidle",   // Estrategia de espera
    "timeout": 30000,              // Timeout en milisegundos
    "parallel": false,             // Procesamiento paralelo
    "max_concurrent": 3,           // MÃ¡ximo de tareas simultÃ¡neas
    "delay_between_requests": 1000, // Pausa entre requests (ms)
    "retry_attempts": 2            // NÃºmero de reintentos
  },
  "markdown": {
    "strip_elements": [            // Elementos HTML a remover
      "script", "style", "nav", "footer"
    ],
    "file_extension": ".md",       // ExtensiÃ³n de archivos de salida
    "naming_pattern": "contenido_{index}", // PatrÃ³n de nombres
    "smart_naming": true,          // Nombres basados en contenido URL
    "clean_excessive_whitespace": true // Limpieza de espacios
  },
  "logging": {
    "level": "INFO",               // Nivel de logging
    "console": true,               // Mostrar en consola
    "file": true,                  // Guardar en archivo
    "log_dir": "logs"              // Directorio de logs
  }
}
```

### Opciones de Wait Until
- `"load"` - Espera carga bÃ¡sica del HTML
- `"domcontentloaded"` - Espera construcciÃ³n del DOM
- `"networkidle"` - Espera que no haya actividad de red (recomendado)

### Niveles de Logging
- `"DEBUG"` - InformaciÃ³n muy detallada
- `"INFO"` - InformaciÃ³n general (recomendado)
- `"WARNING"` - Solo advertencias y errores
- `"ERROR"` - Solo errores crÃ­ticos

## ğŸ“Š EstadÃ­sticas y Monitoreo

El scraper genera estadÃ­sticas detalladas:

```
ğŸ“Š ESTADÃSTICAS FINALES DEL PROCESAMIENTO
============================================================
ğŸ“„ Total de archivos procesados: 5
âœ… Exitosos: 4
âŒ Fallidos: 1
ğŸ“ˆ Tasa de Ã©xito: 80.0%
ğŸ“ Total de palabras extraÃ­das: 15,420
ğŸ“ Total de caracteres: 98,750
â±ï¸ Tiempo total: 45.32 segundos
âš¡ Tiempo promedio por archivo: 11.33s
ğŸ“Š Palabras promedio por archivo: 3,855
============================================================
```

AdemÃ¡s, se guarda un archivo JSON con estadÃ­sticas completas en `estadisticas_procesamiento.json`.

## ğŸ› ï¸ CaracterÃ­sticas Avanzadas

### 1. **Manejo Inteligente de Errores**
- Reintentos automÃ¡ticos en caso de fallos temporales
- Logging detallado de todos los errores
- ContinuaciÃ³n del procesamiento aunque fallen algunos archivos

### 2. **OptimizaciÃ³n de Recursos**
- ReutilizaciÃ³n del navegador entre mÃºltiples URLs
- GestiÃ³n eficiente de memoria
- ConfiguraciÃ³n de timeouts apropiados

### 3. **Nombres de Archivo Inteligentes**
Con `smart_naming: true`, los archivos se nombran basÃ¡ndose en el contenido de la URL:
- `archivo.html#introduccion` â†’ `introduccion_01.md`
- `archivo.html#tema1` â†’ `tema1_02.md`

### 4. **ValidaciÃ³n Completa**
- VerificaciÃ³n de existencia de archivos locales
- ValidaciÃ³n de permisos de escritura
- ComprobaciÃ³n de formato de configuraciÃ³n

### 5. **Procesamiento Paralelo**
Para URLs remotas, permite procesamiento simultÃ¡neo con control de concurrencia:
```bash
python html_scraper_mejorado.py --config config.json --parallel
```

## ğŸ”§ SoluciÃ³n de Problemas

### Error: "playwright command not found"
```bash
pip install playwright
playwright install
```

### Error: "Permission denied"
Cambiar el directorio de salida a uno con permisos:
```json
{
  "output_dir": "~/Documents/markdown-files"
}
```

### Archivos Markdown vacÃ­os
- Verificar que las URLs sean correctas
- Aumentar el timeout si las pÃ¡ginas tardan en cargar
- Usar `--verbose` para ver logs detallados

### Procesamiento muy lento
- Reducir `delay_between_requests`
- Cambiar `wait_until` a `"domcontentloaded"`
- Usar procesamiento paralelo para URLs remotas

## ğŸ“ Estructura de Salida

```
salida_markdown/
â”œâ”€â”€ introduccion_01.md
â”œâ”€â”€ tema1_02.md
â”œâ”€â”€ tema2_03.md
â”œâ”€â”€ estadisticas_procesamiento.json
â””â”€â”€ logs/
    â””â”€â”€ scraper_20250105_143022.log
```

## ğŸ¤ Contribuciones

Este es un script mejorado basado en cÃ³digo original. Las mejoras incluyen:

- Arquitectura orientada a objetos
- ConfiguraciÃ³n flexible
- Manejo robusto de errores
- Logging profesional
- Optimizaciones de rendimiento
- Interfaz de lÃ­nea de comandos completa

## ğŸ“„ Licencia

Este cÃ³digo es una versiÃ³n mejorada del script original con fines educativos y de mejora de herramientas de productividad.

---

Â¡Disfruta de tu herramienta de scraping profesional! ğŸ‰